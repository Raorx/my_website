+++
title = "16S流程重构建"
date = 2018-07-27T09:39:35+08:00
draft = false

# Tags: can be used for filtering projects.
# Example: `tags = ["machine-learning", "deep-learning"]`
tags = ['16S rDNA测序']

# Project summary to display on homepage.
summary = ""

# Optional image to display on homepage.
image_preview = ""

# Optional external URL for project (replaces project detail page).
external_link = ""

# Does the project detail page use math formatting?
math = false

# Does the project detail page use source code highlighting?
highlight = true

# Featured image
# Place your image in the `static/img/` folder and reference its filename below, e.g. `image = "example.jpg"`.
[header]
image = ""
caption = ""

+++

## 第一部分： reads -> contigs、demux并统计

脚本start2SummaryContigs.batch的内容：

```shell
make.contigs(ffastq=LZW113_1.fastq, rfastq=LZW113_2.fastq, oligos=oligos.txt, bdiffs=1, checkorient=t, processors=56)
summary.seqs(fasta=LZW113_1.trim.contigs.fasta)
```

这里注意几点：

1. 流程之后是肯定要用[Nextflow](https://www.nextflow.io/)重构的。比如这里的LZW113_1.fastq等等文件名将来会作为参数传递进来的。
2. SOP里面使用了一个`make.file()`去生成数据文件列表。但是经过实测，这一用法配合`oligos`参数去demux的话，是无法输出正确的结果的（双向fastq程序傻傻分不清楚）。所以**只能在`make.contigs()`手动指定`ffastq`和`rfastq`**。
3. 其实无需后面再通过trim.seqs()中指定`oligos`参数去demux，这一步**（`make.contigs()`）就支持指定`oligos`参数来提供barcodes信息，可以直接完成demux**，也减少了下游分析中可能出现的错误。
4. 应该通过设置`checkorient=t`来开启反向检测，如果出现reads和barcode的方向不一致（获得的正确序列很少），那么mothur会自动反向查询。
5. 这一步中，可以直接设置`allfiles=t`来将完成demux的序列分开文件输出。虽然mothur有groups文件，但是这比较方便后期直接针对单样本进行分析。本例中不开启这个参数，本质上后期也可以`split.groups()`来完成这个操作。
6. 有关`oligos`参数的问题。

    序列比对这个工作里面有一个常识，对于目前最普遍的Illumina系列产品为代表的二代测序而言，通常大家能够容忍的程度是6%，也就是，pair-ends测序，100个碱基内希望见到不超过6个碱基的错误。我们的barcode有多长？8个碱基。这里面如果一个错误都不允许，我觉得有点苛刻，那么1个碱基的错配，我认为是可以接受的。至于primers，我们用的测序接头有多长？才19个碱基啊！好吧我算双端是19 * 2 = 38（其实只能看到3'接头，已经算多了，不要紧），38 * 0.06 = 2.28个碱基，这样算起来，指定2个错配（`pdiffs=2`）已经很过分了。

    所以我就用`pdiffs=2`尝试了一次，发现：

    ```pre
    mothur > summary.seqs(fasta=LZW113_1.trim.contigs.fasta)

    Using 56 processors.

            Start	End	NBases	Ambigs	Polymer	NumSeqs
    Minimum:	1	312	312	0	3	1
    2.5%-tile:	1	371	371	0	4	79
    25%-tile:	1	374	374	0	4	781
    Median: 	1	375	375	0	5	1561
    75%-tile:	1	376	376	0	6	2341
    97.5%-tile:	1	378	378	2	7	3043
    Maximum:	1	409	409	7	9	3121
    Mean:	1	374	374	0	4
    # of Seqs:	3121

    It took 1 secs to summarize 3121 sequences.
    ```

    纳尼？只保留了3121条序列。数据都没了？

    这个时候我做了一些思考：

    原流程内`pdiffs=4`，这意味着什么？我们都知道，Illumina测序使用的接头是用blast算法千挑万选得到的、保证在各个物种内没有相似序列并且长度合适的短序列，但是**4个碱基的错配，已经无法保证其唯一性**，也就是说，我们的16S rDNA测序结果中会有大量相似的序列被匹配出来。

    那么这些序列会被mothur一一剔除吗？不。恰恰相反，mothur会认为这部分序列才是“有效序列”，从而从这部分序列中将“疑似为接头序列”的部分切掉，保留剩余的部分，且只输出这种reads。也就是说，使用原流程去分析，数据中所有不包含“与接头足够相似（错配在4个以内）”的序列的reads都被干掉了。**造成浪费的同时，其实引入了极大的bias，已经是错误的分析了**。经过思考，有了这样的假设，我认为：

    应该在`oligos`参数指向的文件中（本例为oligos.txt）[剔除primers行](https://www.mothur.org/wiki/Oligos_File)，并且`make.configs()`中不再需要指定`pdiffs`参数（允许的primers错配碱基数）。因为数据已经为clean data，如果不是，也推荐用fastp去clean，而不是用mothur（只考虑质量数和接头序列两个特征，往往是不足够的，也没有详细的可视化报告）。

    那么具体实施起来，使用在`oligos`参数指向的文件中保留primers行，并且使用`pdiffs`参数“再去一次接头”会导致什么呢？我做了一系列的小实验。
    先看用mothur再去一次primers的统计结果（按照原来的`pdiffs=4`)：

    ```pre
    mothur > summary.seqs(fasta=LZW113_1.trim.contigs.fasta)

    Using 56 processors.

            Start	End	NBases	Ambigs	Polymer	NumSeqs
    Minimum:	1	181	181	0	3	1
    2.5%-tile:	1	371	371	0	4	200724
    25%-tile:	1	374	374	0	4	2007236
    Median: 	1	376	376	0	5	4014471
    75%-tile:	1	376	376	0	5	6021706
    97.5%-tile:	1	379	379	2	7	7828217
    Maximum:	1	441	441	47	10	8028940
    Mean:	1	375	375	0	4
    # of Seqs:	8028940

    It took 41 secs to summarize 8028940 sequences.
    ```

    注意这里面的8028940，接下来我剔除primers行，以及去掉`pdiffs`参数（其实mothur没有检测到primers行就可以了，这个去不去掉都无所谓），再跑一次：

    ```pre
    mothur > summary.seqs(fasta=LZW113_1.trim.contigs.fasta)

    Using 56 processors.

            Start	End	NBases	Ambigs	Polymer	NumSeqs
    Minimum:	1	210	210	0	3	1
    2.5%-tile:	1	414	414	0	4	208590
    25%-tile:	1	417	417	0	4	2085899
    Median: 	1	419	419	0	5	4171798
    75%-tile:	1	419	419	0	5	6257697
    97.5%-tile:	1	422	422	2	7	8135006
    Maximum:	1	484	484	58	10	8343595
    Mean:	1	418	418	0	4
    # of Seqs:	8343595

    It took 42 secs to summarize 8343595 sequences.
    ```

    现在保留了8343595条序列。8343595 - 8028940 = 314655条序列，也就是说，这个错误的设计，**浪费了314655条高质量序列，且造成了严重了bias**。

    看到这里，也许会有疑问：**测序末端质量低，难道不应该对primers容忍多点吗？**
    
    没错，我也考虑到了这个问题，然而公司的cleaning还算是相当负责了（也和他们的程序的暴力逻辑有关，见到接头或者低质量序列，直接成对的reads全杀掉），**我手动用`grep`检查过，干干净净**...

那么，流程的第一部分大概就这样。之所以选择在这里截断流程一次，是因为summary之后需要根据统计得到的reads的长度分布去修改下一阶段使用的脚本内的参数（比如`maxlength`）。
    
使用时，可以比如`nohup mothur start2SummaryContigs.batch > step1.log &`这样。

## 第二部分： 提取高变区对应的参考序列

这部分参考[上次组会汇报的内容]({{< ref "talk/20180721-16s.zh.md" >}})。

## 第三部分： 过滤、去冗余、计数、比对并统计

脚本screen2SummaryCounts.batch的内容：

```shell
screen.seqs(fasta=LZW113_1.trim.contigs.fasta, group=LZW113_1.contigs.groups, summary=LZW113_1.trim.contigs.summary, maxambig=0, maxlength=450)
summary.seqs()
unique.seqs(fasta=LZW113_1.trim.contigs.good.fasta)
count.seqs(name=LZW113_1.trim.contigs.good.names, group=LZW113_1.contigs.good.groups)
summary.seqs(count=LZW113_1.trim.contigs.good.count_table)
align.seqs(fasta=LZW113_1.trim.contigs.good.unique.fasta, reference=silva.v45.fasta)
summary.seqs(fasta=LZW113_1.trim.contigs.good.unique.align, count=LZW113_1.trim.contigs.good.count_table)
```

这里，在summary的时候，有可能出现这一错误：

```pre
[ERROR]: Your count file contains 6400841 unique sequences, but your fasta file contains 3777293. File mismatch detected, quitting command.
```

很有可能是**data clean或demux的过程出现了问题**，导致很多序列比对不上去。如果排除这些原因，可以这样来继续：

```shell
perl -lanE 'if ($switch) {say if (/^Re/ || exists $need{$F[0]})} else {$need{$1}++ if />(.+)$/} $switch++ if eof(ARGV)' stability.trim.contigs.good.unique.align stability.trim.contigs.good.count_table > stability.trim.contigs.good.filter.count_table
mv stability.trim.contigs.good.count_table stability.trim.contigs.good.count_table.bak
mv stability.trim.contigs.good.filter.count_table stability.trim.contigs.good.count_table
```

因为count table的格式：

```pre
Representative_Sequence total   LZW113
FCAVD1Y_1_1112_19272_5098       1       1
FCAVD1Y_1_2105_6942_8265        1037    1037
FCAVD1Y_1_1113_27631_16492      1       1
FCAVD1Y_1_2110_13690_19012      1       1
FCAVD1Y_1_2113_9095_17142       12      12
FCAVD1Y_1_2107_11955_8783       1       1
FCAVD1Y_1_1104_12000_5514       1       1
FCAVD1Y_1_1113_19468_4747       1       1
FCAVD1Y_1_2102_16601_17755      1       1
```

而alignment的ID格式（类fasta文件）：

```shell
grep ">" stability.trim.contigs.good.unique.align |head
```

```pre
>FCAVD1Y_1_1112_19272_5098
>FCAVD1Y_1_2105_6942_8265
>FCAVD1Y_1_1113_27631_16492
>FCAVD1Y_1_2110_13690_19012
>FCAVD1Y_1_2113_9095_17142
>FCAVD1Y_1_2107_11955_8783
>FCAVD1Y_1_1104_12000_5514
>FCAVD1Y_1_1113_19468_4747
>FCAVD1Y_1_2102_16601_17755
>FCAVD1Y_1_2103_6545_6854
```

所以如果序列数目不一致的话，是可以把count table剪一剪拿去用的。

过长的contigs一般是由于拼接错误，需要去掉。因此`screen.seqs()`中的`maxlength`参数值建议选择97.5%-tile而小于Maximum的某个值（这样至多损失2.5%不到的数据，却可以去掉多数异常的序列）。

使用`nohup mothur screenContigs2SummaryAlignments.batch > step2.log &`完成这一部分的分析。

比对结束后，如果见到类似这种警告信息，是正常的：

```pre
[WARNING]: 1145 of your sequences generated alignments that eliminated too many bases, a list is provided in LZW113_1.trim.contigs.good.unique.flip.accnos.
```

**少量**的序列比对不上没什么问题。如果很多，就要检查一下了。

此外，应该注意汇报的alignments长度，以便于确定下一步的长度控制：

```pre
mothur > summary.seqs(fasta=LZW113_1.trim.contigs.good.unique.align, count=LZW113_1.trim.contigs.good.count_table)

Using 56 processors.

		Start	End	NBases	Ambigs	Polymer	NumSeqs
Minimum:	1	5	2	0	1	1
2.5%-tile:	2	11344	299	0	4	188178
25%-tile:	2	11344	302	0	4	1881778
Median: 	2	11344	304	0	4	3763556
75%-tile:	2	11344	304	0	5	5645333
97.5%-tile:	2	11344	308	0	7	7338933
Maximum:	11340	11344	388	0	9	7527110
Mean:	4	11343	303	0	4
# of unique seqs:	4693900
total # of seqs:	7527110

It took 539 secs to summarize 7527110 sequences.
```

这里很明显[2, 11344]是正确的区间。进入下一步。

## 第四部分：余下分析

